{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ramos_Estevan_lab3.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "CeUjWJuxhDFE",
        "JKk0fBAlVxP3",
        "hXCJEmgpWHxF",
        "qXwQRQHgWdkU",
        "vEDgJW76XSle",
        "_a6m6oDkWyVg",
        "yPNOzGJeXqzJ"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EstevanRamos/ML_Image_Classification/blob/main/Ramos_Estevan_lab3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeUjWJuxhDFE"
      },
      "source": [
        "# **CS 4361/5361 Machine Learning**\n",
        "\n",
        "**Lab 3 - Image Classification**\n",
        "\n",
        "BY: Estevan Ramos\n",
        "\n",
        "**Due:** November 15, 2021<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2qkGd1Ttp1N"
      },
      "source": [
        "In this lab, you will implement several different classifiers to classify the following datasets:\n",
        "\n",
        "*   CIFAR-10, available from tensorflow\n",
        "*   Labeled Faces in the Wild (LFW), available from Sklearn\n",
        "*   CelebA, available from Kaggle\n",
        "\n",
        "For each of the datasets, try to obtain the best possible accuracy using each of the following approaches:\n",
        "\n",
        "*   A non-neural algorithm from Sklearn\n",
        "*   A dense network, implemented using tensorflow\n",
        "*   A convolutional network, implemented using tensorflow\n",
        "\n",
        "Some ideas to improve your results include but are not limited to:\n",
        "\n",
        "*   Preprocessing with principal component analysis \n",
        "*   Parameter search\n",
        "*   Data augmentation. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKk0fBAlVxP3"
      },
      "source": [
        "# Imports and methods to run before classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3I3x07e-2tA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33bc626b-b3ca-49bb-fee9-396d4ba85881"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import distutils\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.activations import *\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5iwVPSLw6-Z"
      },
      "source": [
        "def get_data(dataset):\n",
        "  if dataset == 'CIFAR-10':\n",
        "    #get data\n",
        "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "    #convert to float 32\n",
        "    x_train = x_train.astype('float32') / 255\n",
        "    x_test = x_test.astype('float32') / 255\n",
        "\n",
        "  elif dataset == 'LFW':\n",
        "    from sklearn.datasets import fetch_lfw_people\n",
        "    #get data\n",
        "    lfw_people = fetch_lfw_people(min_faces_per_person=50, resize=0.5)\n",
        "    #convert to float32\n",
        "    X = lfw_people.images/255\n",
        "    y = lfw_people.target\n",
        "    #split into training and testing\n",
        "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=4361)\n",
        "\n",
        "  elif dataset == 'celeb_a':\n",
        "    #get data from drive\n",
        "    from zipfile import ZipFile\n",
        "    file_name = '/content/drive/MyDrive/Machine Learning/img_align_celeba.zip'\n",
        "    #for half res\n",
        "    #file_name = '/content/drive/MyDrive/Machine Learning/img_align_half_res.zip'\n",
        "\n",
        "    #extract imgs to folder\n",
        "    with ZipFile(file_name, 'r') as zip:\n",
        "      zip.extractall('celebA_imgs')\n",
        "      \n",
        "    #extract info to dataframe\n",
        "    df = pd.read_excel('/content/drive/MyDrive/Machine Learning/image_class.xlsx')\n",
        "    #Split df into testing and training\n",
        "    train, test = train_test_split(df, test_size=0.3)\n",
        "\n",
        "    #get img data generator\n",
        "    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "    datagen = ImageDataGenerator(rescale=1./255,)\n",
        "\n",
        "    #build data generators\n",
        "    target_size = (178,218)\n",
        "    #target_size = (109,89) Half Res\n",
        "    train_generator = datagen.flow_from_dataframe(train, target_size=target_size,directory='celebA_imgs/img_align_celeba',batch_size=256,)\n",
        "    test_generator = datagen.flow_from_dataframe(test,target_size= target_size,directory='celebA_imgs/img_align_celeba',batch_size=256, )\n",
        "    return train_generator, test_generator\n",
        "    \n",
        "  else:\n",
        "    print(\"Dataset not found\")\n",
        "    return\n",
        "  return x_train, y_train, x_test , y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROvwbr5lw6wl"
      },
      "source": [
        "def get_model(model_name):\n",
        "  if model_name in ['SVC','DT','NB','KNN' , 'RF']:\n",
        "    if model_name =='SVC':\n",
        "      from sklearn.svm import SVC\n",
        "      model = SVC(C = 1,kernel = 'poly')\n",
        "\n",
        "    elif model_name == 'DT':\n",
        "      from sklearn.tree import DecisionTreeClassifier\n",
        "      model = DecisionTreeClassifier(max_depth = 10)\n",
        "\n",
        "    elif model_name == 'NB':\n",
        "      from sklearn.naive_bayes import BernoulliNB\n",
        "      model = BernoulliNB()\n",
        "\n",
        "    elif model_name == 'KNN':\n",
        "      from sklearn.neighbors import KNeighborsClassifier\n",
        "      model = KNeighborsClassifier(n_neighbors = 3 , weights = 'distance')\n",
        "\n",
        "    elif model_name == 'RF':\n",
        "      from sklearn.ensemble import RandomForestClassifier\n",
        "      model = RandomForestClassifier(n_estimators = 20, max_depth=2, random_state=0)\n",
        "\n",
        "  else:\n",
        "    print('Unknown model')\n",
        "    return \n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqMraFvEpjzW"
      },
      "source": [
        "def run_model(dataset_name , model_name):\n",
        "  x_train , y_train , x_test, y_test = get_data(dataset_name)\n",
        "  #convert too float32 and reshape\n",
        "  x_train = np.float32(x_train/255).reshape(x_train.shape[0],-1)\n",
        "  x_test = np.float32(x_test/255).reshape(x_test.shape[0],-1)\n",
        "  #get model;\n",
        "  model = get_model(model_name)\n",
        "  model.fit(x_train, y_train.reshape(-1))\n",
        "  #get prediction\n",
        "  pred = model.predict(x_test)\n",
        "  accuracy = accuracy_score(y_test, pred)\n",
        "  print(\"The accuracy of \", model_name ,\"on dataset\", dataset_name,\"is:\", accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkJY70skJheI"
      },
      "source": [
        "def plot_results(all_history):\n",
        "  loss, val_loss, accuracy, val_accuracy = [], [], [], []\n",
        "  for history in all_history:\n",
        "    loss += history.history['loss']\n",
        "    val_loss += history.history['val_loss']\n",
        "    accuracy += history.history['accuracy']\n",
        "    val_accuracy += history.history['val_accuracy']\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.plot(accuracy,label = 'train')\n",
        "  ax.plot(val_accuracy,label = 'test')\n",
        "  ax.set_title('Accuracy')\n",
        "  ax.legend(loc='lower right')\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.plot(loss,label = 'train')\n",
        "  ax.plot(val_loss,label = 'test')\n",
        "  ax.set_title('Loss')\n",
        "  ax.legend(loc='upper right')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXCJEmgpWHxF"
      },
      "source": [
        "# Sklearn Algorithims and accuracies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAHJePxX_Plv"
      },
      "source": [
        "models = ['DT','NB','RF']\n",
        "for m in models:\n",
        "  run_model('CIFAR-10', m)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFDA5ItdiY5N"
      },
      "source": [
        "The accuracy of  DT on dataset CIFAR-10 is: 0.3063\n",
        "\n",
        "The accuracy of  NB on dataset CIFAR-10 is: 0.1236\n",
        "\n",
        "The accuracy of  RF on dataset CIFAR-10 is: 0.2562"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkMMrf0L-d_4"
      },
      "source": [
        "models = ['SVC','DT','NB','KNN' , 'RF']\n",
        "for m in models:\n",
        "  run_model('LFW', m)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHLVCASPX87z"
      },
      "source": [
        "The accuracy of  SVC on dataset LFW is: 0.8384\n",
        "\n",
        "The accuracy of  DT on dataset LFW is: 0.4256\n",
        "\n",
        "The accuracy of  NB on dataset LFW is: 0.3564\n",
        "\n",
        "The accuracy of  KNN on dataset LFW is: 0.5487\n",
        "\n",
        "The accuracy of  RF on dataset LFW is: 0.3948"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXwQRQHgWdkU"
      },
      "source": [
        "# Get Data and Info from datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcAlxCZal_U2"
      },
      "source": [
        "dataset_name = 'CIFAR-10'\n",
        "x_train, y_train, x_test , y_test = get_data(dataset_name)\n",
        "#get Num classes and shape\n",
        "num_classes = np.max(y_train)+1\n",
        "input_shape = x_train.shape[1:]\n",
        "#Change Y to Onehot\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes = num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes = num_classes)\n",
        "#epochs and batchsize\n",
        "epochs = 50\n",
        "batch_size = 256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jACA1xQCrKz"
      },
      "source": [
        "dataset_name = 'LFW'\n",
        "#get data\n",
        "x_train, y_train, x_test , y_test = get_data(dataset_name)\n",
        "#Expand dimesion for CNN classification\n",
        "x_train = np.float32(np.expand_dims(x_train, -1))\n",
        "x_test = np.float32(np.expand_dims(x_test, -1))\n",
        "#get Num classes and shape\n",
        "num_classes = np.max(y_train)+1\n",
        "input_shape = x_train.shape[1:]\n",
        "#Change Y to Onehot\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes = num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes = num_classes)\n",
        "\n",
        "#epochs and batchsize\n",
        "epochs = 50\n",
        "batch_size = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYlWDUUn_wm2",
        "outputId": "a1ef8853-5afb-4d42-aaf0-77bc59ceb561"
      },
      "source": [
        "dataset_name = 'celeb_a'\n",
        "#get data\n",
        "train_generator, test_generator = get_data(dataset_name)\n",
        "#Num classes and shape\n",
        "num_classes = 2\n",
        "input_shape = (178,218,3)\n",
        "#batch size and epochs\n",
        "epochs =  10\n",
        "batch_size= 8192"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 141819 validated image filenames belonging to 2 classes.\n",
            "Found 60780 validated image filenames belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEDgJW76XSle"
      },
      "source": [
        "# Dense Network Classification and Accuracies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZC0bZ-_cx9A"
      },
      "source": [
        "x_train = x_train.reshape(x_train.shape[0],-1)\n",
        "x_test = x_test.reshape(x_test.shape[0],-1)\n",
        "\n",
        "inputs = x_train.shape[1] \n",
        "\n",
        "if dataset_name == 'CIFAR-10':\n",
        "  hidden_1 = 500\n",
        "  hidden_2 = 500\n",
        "  hidden_3 = 200\n",
        "\n",
        "  epochs = 30\n",
        "  batch_size = 256\n",
        "\n",
        "if dataset_name == 'LFW':\n",
        "  hidden_1 = 200\n",
        "  hidden_2 = 200\n",
        "  hidden_3 = 100\n",
        "  \n",
        "  epochs = 50\n",
        "  batch_size = 256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWnqI5gD-8KU"
      },
      "source": [
        "def dense_model(inputs = 784,hidden_1=500, hidden_2=500 ,classes = 10):\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(Dense(hidden_1, input_shape=(inputs,), activation='relu'))\n",
        "  model.add(Dense(hidden_2, activation='relu'))\n",
        "  model.add(Dense(classes, activation='softmax'))\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JU3oJ_julFd7"
      },
      "source": [
        "dense_network = dense_model2(inputs = inputs, hidden_1 = hidden_1, hidden_2 = hidden_2, hidden_3 = hidden_3 ,classes = num_classes)\n",
        "dense_network.summary() \n",
        "dense_network.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "#dense_network.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "all_history = []\n",
        "\n",
        "history = dense_network.fit(\n",
        "    x_train, y_train,\n",
        "    epochs =  epochs, \n",
        "    batch_size=batch_size, \n",
        "    verbose = 1,\n",
        "    validation_data=(x_test, y_test),\n",
        ")\n",
        "\n",
        "all_history.append(history)\n",
        "\n",
        "print('Final accuracy on training set: {:.4f}'.format(history.history['accuracy'][-1]))\n",
        "print('Final accuracy on test set: {:.4f}'.format(history.history['val_accuracy'][-1]))\n",
        "print('Mean accuracy in last 5 epochs: {:.4f}'.format(np.mean(history.history['val_accuracy'][-5:])))\n",
        "plot_results(all_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSWgcrO13lBl"
      },
      "source": [
        "Accuracy on CIFAR-10: .4689\n",
        "\n",
        "Accuracy on LFW: 0.4374"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JofWmMFQvNo_"
      },
      "source": [
        "dense_network = dense_model(inputs = inputs, hidden_1 = hidden_1, hidden_2 = hidden_2, classes = num_classes)\n",
        "dense_network.summary() \n",
        "#dense_network.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "dense_network.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "all_history = []\n",
        "\n",
        "history = dense_network.fit(\n",
        "    x_train, y_train,\n",
        "    epochs =  epochs, \n",
        "    batch_size= batch_size, \n",
        "    verbose = 1,\n",
        "    validation_data=(x_test, y_test),\n",
        ")\n",
        "\n",
        "all_history.append(history)\n",
        "\n",
        "print('Final accuracy on training set: {:.4f}'.format(history.history['accuracy'][-1]))\n",
        "print('Final accuracy on test set: {:.4f}'.format(history.history['val_accuracy'][-1]))\n",
        "print('Mean accuracy in last 5 epochs: {:.4f}'.format(np.mean(history.history['val_accuracy'][-5:])))\n",
        "plot_results(all_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9Fh6P8Fr21W"
      },
      "source": [
        "def dense_model2(inputs = 784,hidden_1=500, hidden_2=500 ,hidden_3 = 500,classes = 10, dropout_rate = 0.1):\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(Dense(hidden_1, input_shape=(inputs,), activation='relu'))\n",
        "  model.add(Dropout(dropout_rate))\n",
        "  model.add(Dense(hidden_2, activation='relu'))\n",
        "  model.add(Dropout(dropout_rate))\n",
        "  model.add(Dense(hidden_3, activation='relu'))\n",
        "  model.add(Dropout(dropout_rate))\n",
        "  model.add(Dense(classes, activation='softmax'))\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xctcuZxi_LJJ"
      },
      "source": [
        "dense_network = dense_model2(inputs = inputs, hidden_1 = hidden_1, hidden_2 = hidden_2, hidden_3 = hidden_3 ,classes = num_classes)\n",
        "dense_network.summary() \n",
        "#dense_network.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "dense_network.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "all_history = []\n",
        "\n",
        "history = dense_network.fit(\n",
        "    x_train, y_train,\n",
        "    epochs =  epochs, \n",
        "    batch_size=batch_size, \n",
        "    verbose = 1,\n",
        "    validation_data=(x_test, y_test),\n",
        ")\n",
        "\n",
        "all_history.append(history)\n",
        "\n",
        "print('Final accuracy on training set: {:.4f}'.format(history.history['accuracy'][-1]))\n",
        "print('Final accuracy on test set: {:.4f}'.format(history.history['val_accuracy'][-1]))\n",
        "print('Mean accuracy in last 5 epochs: {:.4f}'.format(np.mean(history.history['val_accuracy'][-5:])))\n",
        "plot_results(all_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URTfD3M_BNKO"
      },
      "source": [
        "Accuracy on CIFAR-10: 0.5227\n",
        "\n",
        "Accuracy on LFW:  0.5173"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fknKZaW6sAwI"
      },
      "source": [
        "def dense_model3(inputs = 784,hidden_1=500, hidden_2=500 ,hidden_3 = 500,classes = 10, dropout_rate = 0.1):\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(Dense(hidden_1, input_shape=(inputs,), activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(dropout_rate))\n",
        "  model.add(Dense(hidden_2, activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(dropout_rate))\n",
        "  model.add(Dense(hidden_3, activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(dropout_rate))\n",
        "  model.add(Dense(classes, activation='softmax'))\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJs6XAsZCX_V"
      },
      "source": [
        "dense_network = dense_model3(inputs = inputs, hidden_1 = hidden_1, hidden_2 = hidden_2, hidden_3 = hidden_3 , classes = num_classes)\n",
        "dense_network.summary() \n",
        "#dense_network.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "dense_network.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "all_history = []\n",
        "\n",
        "history = dense_network.fit(\n",
        "    x_train, y_train,\n",
        "    epochs =  epochs, \n",
        "    batch_size=batch_size, \n",
        "    verbose = 1,\n",
        "    validation_data=(x_test, y_test),\n",
        ")\n",
        "\n",
        "all_history.append(history)\n",
        "\n",
        "print('Final accuracy on training set: {:.4f}'.format(history.history['accuracy'][-1]))\n",
        "print('Final accuracy on test set: {:.4f}'.format(history.history['val_accuracy'][-1]))\n",
        "print('Mean accuracy in last 5 epochs: {:.4f}'.format(np.mean(history.history['val_accuracy'][-5:])))\n",
        "plot_results(all_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-klUQnjkX_Na"
      },
      "source": [
        "Accuracy on CIFAR-10: 0.5014\n",
        "\n",
        "Accuracy on LFW:0.6672\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_a6m6oDkWyVg"
      },
      "source": [
        "# Convolution Neural Network Classification and Accuracies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDP_sdIcJpPf"
      },
      "source": [
        "def vgg3(input_shape=(32,32,3), classes = 10):\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(Conv2D(32, kernel_size=(3, 3), input_shape=input_shape, padding='same', activation=\"relu\"))\n",
        "  model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "  model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  model.add(Dense(classes, activation='softmax'))\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7HUwfPqNb5k"
      },
      "source": [
        "cnn = vgg3(input_shape= input_shape, classes=num_classes)\n",
        "cnn.summary() \n",
        "cnn.compile(loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "all_history = []\n",
        "\n",
        "print('Dataset:',dataset_name)\n",
        "    \n",
        "history = cnn.fit(\n",
        "    train_generator,\n",
        "    epochs =  epochs, \n",
        "    batch_size = batch_size, \n",
        "    verbose = 1,\n",
        "    validation_data= test_generator,\n",
        ")\n",
        "\n",
        "all_history.append(history)\n",
        "\n",
        "print('Final accuracy on training set: {:.4f}'.format(history.history['accuracy'][-1]))\n",
        "print('Final accuracy on test set: {:.4f}'.format(history.history['val_accuracy'][-1]))\n",
        "print('Mean accuracy in last 5 epochs: {:.4f}'.format(np.mean(history.history['val_accuracy'][-5:])))\n",
        "\n",
        "plot_results(all_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKDsXL_YdJiM"
      },
      "source": [
        "Accuracy on CIFAR-10: .7292\n",
        "\n",
        "Accuracy on LFW: 0.8574\n",
        "\n",
        "Celeba: 0.9713"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8kKBc1S96SM"
      },
      "source": [
        "def vgg4(input_shape=(32,32,3), classes = 10):\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(Conv2D(32, kernel_size=(3, 3), input_shape=input_shape, padding='same', activation=\"relu\"))\n",
        "  model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "  model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "  model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "  model.add(Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  model.add(Dense(classes, activation='softmax'))\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mFKbr6VcLxe"
      },
      "source": [
        "cnn = vgg4(input_shape= input_shape, classes=num_classes)\n",
        "cnn.summary() \n",
        "cnn.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "all_history = []\n",
        "\n",
        "print('Dataset:',dataset_name)\n",
        "if dataset_name == 'celeb_a':\n",
        "  history = cnn.fit(\n",
        "    train_generator,\n",
        "    epochs =  epochs, \n",
        "    batch_size = batch_size, \n",
        "    verbose = 1,\n",
        "    validation_data= test_generator,\n",
        "    )\n",
        "else:\n",
        "  history = cnn.fit(\n",
        "      x_train, y_train,\n",
        "      epochs =  epochs, \n",
        "      batch_size= batch_size,\n",
        "      verbose = 1,\n",
        "      validation_data=(x_test, y_test),\n",
        "      )\n",
        "\n",
        "all_history.append(history)\n",
        "\n",
        "print('Final accuracy on training set: {:.4f}'.format(history.history['accuracy'][-1]))\n",
        "print('Final accuracy on test set: {:.4f}'.format(history.history['val_accuracy'][-1]))\n",
        "print('Mean accuracy in last 5 epochs: {:.4f}'.format(np.mean(history.history['val_accuracy'][-5:])))\n",
        "\n",
        "plot_results(all_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCDOzoZFuRWZ"
      },
      "source": [
        "Accuracy on CIFAR-10: 0.7463\n",
        "\n",
        "Accuracy on LFW:0.8179\n",
        "\n",
        "celeba:  0.9762"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9q-W275K6be"
      },
      "source": [
        "cnn = vgg4(input_shape= input_shape, classes= num_classes)\n",
        "cnn.summary() \n",
        "#cnn.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "cnn.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "all_history = []\n",
        "\n",
        "print('Dataset:',dataset_name)\n",
        "if dataset_name == 'celeb_a':\n",
        "  history = cnn.fit(\n",
        "    train_generator,\n",
        "    epochs =  epochs, \n",
        "    batch_size = batch_size, \n",
        "    verbose = 1,\n",
        "    validation_data= test_generator,\n",
        "    )\n",
        "else:\n",
        "  history = cnn.fit(\n",
        "      x_train, y_train,\n",
        "      epochs =  epochs, \n",
        "      batch_size= batch_size,\n",
        "      verbose = 1,\n",
        "      validation_data=(x_test, y_test),\n",
        "      )\n",
        "\n",
        "\n",
        "all_history.append(history)\n",
        "\n",
        "print('Final accuracy on training set: {:.4f}'.format(history.history['accuracy'][-1]))\n",
        "print('Final accuracy on test set: {:.4f}'.format(history.history['val_accuracy'][-1]))\n",
        "print('Mean accuracy in last 5 epochs: {:.4f}'.format(np.mean(history.history['val_accuracy'][-5:])))\n",
        "\n",
        "plot_results(all_history)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkddzbMey4Ow"
      },
      "source": [
        "Accuracy on CIFAR-10: 0.7691\n",
        "\n",
        "Accuracy on LFW 0.8333"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8osNbnyoLfhe"
      },
      "source": [
        "def vgg4_with_dropout(input_shape=(32,32,3), dropout_rate=0.2 , classes = 10):\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(Conv2D(32, kernel_size=(3, 3), input_shape=input_shape, padding='same', activation=\"relu\"))\n",
        "  model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Dropout(dropout_rate))\n",
        "  model.add(Conv2D(64, kernel_size=(3, 3), padding='same', activation=\"relu\"))\n",
        "  model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Dropout(dropout_rate))\n",
        "  model.add(Conv2D(128, kernel_size=(3, 3), padding='same', activation=\"relu\"))\n",
        "  model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Dropout(dropout_rate))\n",
        "  model.add(Conv2D(256, kernel_size=(3, 3), padding='same', activation=\"relu\"))\n",
        "  model.add(Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Dropout(dropout_rate))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  model.add(Dense(classes, activation='softmax'))\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1piZDatLwSm"
      },
      "source": [
        "cnn = vgg4_with_dropout(input_shape= input_shape,classes = num_classes)\n",
        "cnn.summary() \n",
        "#cnn.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "cnn.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "all_history = []\n",
        "\n",
        "print('Dataset:',dataset_name)\n",
        "if dataset_name == 'celeb_a':\n",
        "  history = cnn.fit(\n",
        "    train_generator,\n",
        "    epochs =  epochs, \n",
        "    batch_size = batch_size, \n",
        "    verbose = 1,\n",
        "    validation_data= test_generator,\n",
        "    )\n",
        "else:\n",
        "  history = cnn.fit(\n",
        "      x_train, y_train,\n",
        "      epochs =  epochs, \n",
        "      batch_size= batch_size,\n",
        "      verbose = 1,\n",
        "      validation_data=(x_test, y_test),\n",
        "      )\n",
        "\n",
        "all_history.append(history)\n",
        "\n",
        "print('Final accuracy on training set: {:.4f}'.format(history.history['accuracy'][-1]))\n",
        "print('Final accuracy on test set: {:.4f}'.format(history.history['val_accuracy'][-1]))\n",
        "print('Mean accuracy in last 5 epochs: {:.4f}'.format(np.mean(history.history['val_accuracy'][-5:])))\n",
        "\n",
        "plot_results(all_history)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8slxoTbYdiU9"
      },
      "source": [
        "Accuracy on CIFAR-10: 0.8114\n",
        "\n",
        "Accuracy on LFW: 0.8656"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNvp48apOzJl"
      },
      "source": [
        "def vgg_dropout_bn(input_shape=(32,32,3),blocks=3,branching_factor=2,first_layer_filters=32,dropout=0.2,dense_layer=128 , classes = 10):\n",
        "  model = tf.keras.models.Sequential()\n",
        "  f = first_layer_filters\n",
        "  for i in range(blocks):\n",
        "    if i==0:\n",
        "      model.add(Conv2D(f, kernel_size=(3, 3), input_shape=input_shape, padding='same', activation=\"relu\"))\n",
        "    else:\n",
        "      model.add(Conv2D(f, kernel_size=(3, 3), padding='same', activation='relu'))\n",
        "    model.add(Conv2D(f, kernel_size=(3, 3), padding='same', activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(dropout))\n",
        "    f = int(f*branching_factor)\n",
        "  model.add(Flatten())\n",
        "  if dense_layer>0:\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(dropout))\n",
        "  model.add(Dense(classes))\n",
        "  model.add(Activation('softmax'))\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6wdscjudxuO"
      },
      "source": [
        "cnn = vgg_dropout_bn(input_shape = input_shape , blocks=3 , classes = num_classes)\n",
        "cnn.summary() \n",
        "#cnn.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "cnn.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "all_history = []\n",
        "\n",
        "print('Dataset:',dataset_name)\n",
        "if dataset_name == 'celeb_a':\n",
        "  history = cnn.fit(\n",
        "    train_generator,\n",
        "    epochs =  epochs, \n",
        "    batch_size = batch_size, \n",
        "    verbose = 1,\n",
        "    validation_data= test_generator,\n",
        "    )\n",
        "else:\n",
        "  history = cnn.fit(\n",
        "      x_train, y_train,\n",
        "      epochs =  epochs, \n",
        "      batch_size= batch_size,\n",
        "      verbose = 1,\n",
        "      validation_data=(x_test, y_test),\n",
        "      )\n",
        "\n",
        "all_history.append(history)\n",
        "print('Final accuracy on training set: {:.4f}'.format(history.history['accuracy'][-1]))\n",
        "print('Final accuracy on test set: {:.4f}'.format(history.history['val_accuracy'][-1]))\n",
        "print('Mean accuracy in last 5 epochs: {:.4f}'.format(np.mean(history.history['val_accuracy'][-5:])))\n",
        "plot_results(all_history)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-f1uIwbHewge"
      },
      "source": [
        "Accuracy on CIFAR-10: 0.8313\n",
        "\n",
        "Accuracy on LFW:  0.9426 Very Inconsistent\n",
        "\n",
        "Accuracy on celebA: 0.9766 Previous were skipped for times sake\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPNOzGJeXqzJ"
      },
      "source": [
        "# Data Augmentation Accuracies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXxkgZAQdcze"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "dataset_name = 'CIFAR-10'\n",
        "#get data\n",
        "x_train, y_train, x_test , y_test = get_data(dataset_name)\n",
        "#reshape data for dense network\n",
        "x_train = x_train.reshape(x_train.shape[0],-1)\n",
        "x_test = x_test.reshape(x_test.shape[0],-1)\n",
        "#create pca\n",
        "n_components = 64\n",
        "pca = PCA(n_components=n_components, svd_solver=\"randomized\", whiten=True).fit(x_train)\n",
        "#transform x train and test\n",
        "x_train_pca = pca.transform(x_train)\n",
        "x_test_pca = pca.transform(x_test)\n",
        "#convert y to One Hot\n",
        "y_train = tf.keras.utils.to_categorical(y_train,num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test,num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRMDsam0iifF"
      },
      "source": [
        "  if dataset_name == 'LFW':\n",
        "    hidden_1 = 200\n",
        "    hidden_2 = 200\n",
        "    hidden_3 = 100\n",
        "    \n",
        "    epochs = 50\n",
        "    batch_size = 256\n",
        "  else:\n",
        "    hidden_1 = 500\n",
        "    hidden_2 = 300\n",
        "    hidden_3 = 200\n",
        "    \n",
        "    epochs = 30\n",
        "    batch_size = 256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsrmDxMAiKIJ"
      },
      "source": [
        "def dense_model3(inputs = 784,hidden_1=500, hidden_2=500 ,hidden_3 = 500,classes = 10, dropout_rate = 0.2):\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(Dense(hidden_1, input_shape=(inputs,), activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(dropout_rate))\n",
        "  model.add(Dense(hidden_2, activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(dropout_rate))\n",
        "  model.add(Dense(hidden_3, activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(dropout_rate))\n",
        "  model.add(Dense(classes, activation='softmax'))\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sl0D2iwXiM_E"
      },
      "source": [
        "dense_network = dense_model3(inputs = n_components, hidden_1 = hidden_1, hidden_2 = hidden_2, hidden_3 = hidden_3 , classes = num_classes)\n",
        "dense_network.summary() \n",
        "#dense_network.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "dense_network.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "all_history = []\n",
        "\n",
        "history = dense_network.fit(\n",
        "    x_train_pca, y_train,\n",
        "    epochs =  epochs, \n",
        "    batch_size=batch_size, \n",
        "    verbose = 1,\n",
        "    validation_data=(x_test_pca, y_test),\n",
        ")\n",
        "\n",
        "all_history.append(history)\n",
        "\n",
        "print('Final accuracy on training set: {:.4f}'.format(history.history['accuracy'][-1]))\n",
        "print('Final accuracy on test set: {:.4f}'.format(history.history['val_accuracy'][-1]))\n",
        "print('Mean accuracy in last 5 epochs: {:.4f}'.format(np.mean(history.history['val_accuracy'][-5:])))\n",
        "plot_results(all_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7SyxH09kTyW"
      },
      "source": [
        "Accuracy on CIFAR-10: 0.5580\n",
        "\n",
        "Accuracy on LFW: 0.8010"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Az0q83F0yhI5"
      },
      "source": [
        "def get_data_gen(dataset_name):\n",
        "  from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "  if dataset_name == 'celeb_a':\n",
        "    #get data from drive\n",
        "    from zipfile import ZipFile\n",
        "    file_name = '/content/drive/MyDrive/Machine Learning/img_align_celeba.zip'\n",
        "    #for half res\n",
        "    #file_name = '/content/drive/MyDrive/Machine Learning/img_align_half_res.zip'\n",
        "\n",
        "    #extract imgs to folder\n",
        "    with ZipFile(file_name, 'r') as zip:\n",
        "      zip.extractall('celebA_imgs')\n",
        "      \n",
        "    #extract info to dataframe\n",
        "    df = pd.read_excel('/content/drive/MyDrive/Machine Learning/image_class.xlsx')\n",
        "    #Split df into testing and training\n",
        "    train, test = train_test_split(df, test_size=0.3)\n",
        "\n",
        "    #get img data generator\n",
        "    datagen = ImageDataGenerator(\n",
        "        rotation_range=5,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        zoom_range=0.2,)\n",
        "    \n",
        "    test_datagen = ImageDataGenerator(rescale=1./255,)\n",
        "\n",
        "    #build data generators\n",
        "    target_size = (178,218)\n",
        "    #target_size = (109,89) Half Res\n",
        "    train_generator = datagen.flow_from_dataframe(train, target_size=target_size,directory='celebA_imgs/img_align_celeba',batch_size=2048,)\n",
        "    test_generator = test_datagen.flow_from_dataframe(test,target_size= target_size,directory='celebA_imgs/img_align_celeba',batch_size=2048, )\n",
        "    return train_generator, test_generator\n",
        "\n",
        "  elif dataset_name == 'LFW':\n",
        "    #get data\n",
        "    x_train, y_train, x_test , y_test = get_data(dataset_name)\n",
        "    #expand dimensions for CNN\n",
        "    x_train = np.float32(np.expand_dims(x_train, -1))\n",
        "    x_test = np.float32(np.expand_dims(x_test, -1))\n",
        "    #convert to One Hot\n",
        "    num_classes = np.max(y_train)+1\n",
        "    y_train = tf.keras.utils.to_categorical(y_train, num_classes = num_classes)\n",
        "    y_test = tf.keras.utils.to_categorical(y_test, num_classes = num_classes)\n",
        "    batch_size = 64\n",
        "    #Image Augmentation\n",
        "    datagen = ImageDataGenerator(\n",
        "        rotation_range=5,\n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0.1,\n",
        "        zoom_range=0.1,)\n",
        "    \n",
        "    #no augmentation for testing\n",
        "    test_datagen = ImageDataGenerator(rescale=1./255,)\n",
        "  else:\n",
        "    #get data\n",
        "    x_train, y_train, x_test , y_test = get_data(dataset_name)\n",
        "    #class and batch size info\n",
        "    batch_size = 256\n",
        "    num_classes = np.max(y_train)+1\n",
        "    #convert to onehot\n",
        "    y_train = tf.keras.utils.to_categorical(y_train, num_classes = num_classes)\n",
        "    y_test = tf.keras.utils.to_categorical(y_test, num_classes = num_classes)\n",
        "\n",
        "    #Image Augmentation\n",
        "    datagen = ImageDataGenerator(\n",
        "        rotation_range=30,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,)\n",
        "    \n",
        "    #no augmentation for testing\n",
        "    test_datagen = ImageDataGenerator(rescale=1./255,)\n",
        "\n",
        "  #create train and test generators\n",
        "  train_generator = datagen.flow(x_train,y_train, batch_size = batch_size)\n",
        "  test_generator = datagen.flow(x_test,y_test , batch_size = batch_size)\n",
        "  \n",
        "  return train_generator, test_generator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bsfzGnxivQh"
      },
      "source": [
        "for Cifair we have many images of varying items, fliping and rotating is going to help a lot in this case but since the LFW and Celeba are images of faces flipping and rotaing is very detrimental. I decided to just rotate a little and zoom but doesnt show any immense improvements and sometimes makes things worse. Some alternatives maybe changing color or brightness might be better"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ssiacQ511Aq"
      },
      "source": [
        "dataset_name = 'CIFAR-10'\n",
        "num_classes = 10\n",
        "input_shape = (32,32,3)\n",
        "epochs = 30\n",
        "batch_size = 256\n",
        "lr = 0.001"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VB0W5F6g10uA"
      },
      "source": [
        "dataset_name = 'LFW'\n",
        "num_classes = 12\n",
        "input_shape = (62,47,1)\n",
        "epochs = 50\n",
        "batch_size = 64\n",
        "lr = 0.0005"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XsdhUAR2NJs"
      },
      "source": [
        "dataset_name = 'celeb_a'\n",
        "num_classes = 2\n",
        "input_shape = (178,218,3)\n",
        "epochs = 6\n",
        "batch_size = 2048\n",
        "lr= 0.001"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6ddipTMp7uO",
        "outputId": "9b89391d-ef29-4294-a4d9-a80a26a92ed5"
      },
      "source": [
        "train_generator , test_generator = get_data_gen(dataset_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 141819 validated image filenames belonging to 2 classes.\n",
            "Found 60780 validated image filenames belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBBLCKVfrtSZ"
      },
      "source": [
        "def vgg_dropout_bn(input_shape=(32,32,3),blocks=3,branching_factor=2,first_layer_filters=32,dropout=0.2,dense_layer=128 , classes = 10):\n",
        "  model = tf.keras.models.Sequential()\n",
        "  f = first_layer_filters\n",
        "  for i in range(blocks):\n",
        "    if i==0:\n",
        "      model.add(Conv2D(f, kernel_size=(3, 3), input_shape=input_shape, padding='same', activation=\"relu\"))\n",
        "    else:\n",
        "      model.add(Conv2D(f, kernel_size=(3, 3), padding='same', activation='relu'))\n",
        "    model.add(Conv2D(f, kernel_size=(3, 3), padding='same', activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(dropout))\n",
        "    f = int(f*branching_factor)\n",
        "  model.add(Flatten())\n",
        "  if dense_layer>0:\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(dropout))\n",
        "  model.add(Dense(classes))\n",
        "  model.add(Activation('softmax'))\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGHFc3MdruJN"
      },
      "source": [
        "cnn = vgg_dropout_bn(input_shape = input_shape , blocks=3 , classes = num_classes)\n",
        "cnn.summary() \n",
        "#cnn.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "cnn.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "all_history = []\n",
        "\n",
        "print('Dataset:',dataset_name)\n",
        "history = cnn.fit(\n",
        "    train_generator,\n",
        "    epochs =  epochs, \n",
        "    batch_size = batch_size,\n",
        "    verbose = 1,\n",
        "    validation_data= test_generator,\n",
        "    )\n",
        "\n",
        "all_history.append(history)\n",
        "print('Final accuracy on training set: {:.4f}'.format(history.history['accuracy'][-1]))\n",
        "print('Final accuracy on test set: {:.4f}'.format(history.history['val_accuracy'][-1]))\n",
        "print('Mean accuracy in last 5 epochs: {:.4f}'.format(np.mean(history.history['val_accuracy'][-5:])))\n",
        "plot_results(all_history)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNV-1t_4E3n8"
      },
      "source": [
        "def vgg3(input_shape=(32,32,3), classes = 10):\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(Conv2D(32, kernel_size=(3, 3), input_shape=input_shape, padding='same', activation=\"relu\"))\n",
        "  model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "  model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  model.add(Dense(classes, activation='softmax'))\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPSWOyTwE-uM"
      },
      "source": [
        "cnn = vgg3(input_shape= input_shape, classes=num_classes)\n",
        "cnn.summary() \n",
        "cnn.compile(loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "all_history = []\n",
        "\n",
        "print('Dataset:',dataset_name)\n",
        "    \n",
        "history = cnn.fit(\n",
        "    train_generator,\n",
        "    epochs =  epochs, \n",
        "    batch_size = batch_size, \n",
        "    verbose = 1,\n",
        "    validation_data= test_generator,\n",
        ")\n",
        "\n",
        "all_history.append(history)\n",
        "\n",
        "print('Final accuracy on training set: {:.4f}'.format(history.history['accuracy'][-1]))\n",
        "print('Final accuracy on test set: {:.4f}'.format(history.history['val_accuracy'][-1]))\n",
        "print('Mean accuracy in last 5 epochs: {:.4f}'.format(np.mean(history.history['val_accuracy'][-5:])))\n",
        "\n",
        "plot_results(all_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LAymFvM0b0E"
      },
      "source": [
        "Accuracy on CIFAR-10: 0.7654\n",
        "\n",
        "Accuracy on LFW: 0.9026\n",
        "\n",
        "Celeba: keeps crashing runtime, not sure why"
      ]
    }
  ]
}